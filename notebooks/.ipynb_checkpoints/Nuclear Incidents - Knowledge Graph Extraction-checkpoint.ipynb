{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Nuclear Incidents\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Knowledge Graph Extraction\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 15px; \n",
    "      text-align: center; \n",
    "      padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Table of Content\n",
    "  </div> \n",
    "\n",
    "\n",
    "**1.** [**Syntactic Tree Documentation**](#nlu)\n",
    "\n",
    "$\\qquad \\bullet$ [Part Of Speech Tags](#nlu_pos)<br>\n",
    "$\\qquad \\bullet$ [Dependency Tags](#nlu_dep)<br>\n",
    "\n",
    "**2.** [**Natural Language Understanding**](#kg)<br>\n",
    "\n",
    "$\\qquad \\bullet$ [Syntactic Dependency Tree](#kg_syntactic_dependency_tree)<br>\n",
    "$\\qquad \\bullet$ [Coreference Resolution](#kg_coref)<br>\n",
    "$\\qquad \\bullet$ [Entity Linking](#kg_entity_linking)<br>\n",
    "\n",
    "**3.** [**Knowledge Graph**](#kg)<br>\n",
    "\n",
    "$\\qquad \\bullet$ [Syntactic Dependency Graph](#kg_syntactic_dependency_graph)<br>\n",
    "$\\qquad \\bullet$ [Edge orientation checking](#kg_orientation)<br>\n",
    "$\\qquad \\bullet$ [Attribute Extraction from descriptive POS tags](#kg_attribute_extraction)<br>\n",
    "$\\qquad \\bullet$ [Relation Extraction from connecting POS tags](#kg_relation_extraction)<br>\n",
    "$\\qquad \\bullet$ [Relation Classification [TODO]](#kg_relation_classification)<br>\n",
    "$\\qquad \\bullet$ [Final Graph construction](#kg_construction)<br>\n",
    "\n",
    "\n",
    "[Bottom](#bottom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open, BytesIO\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "from dateutil.parser import parse\n",
    "from ast import literal_eval # transforms back stings into list of words\n",
    "import dill\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# for graphs\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# for clustering\n",
    "from sklearn.decomposition import PCA, NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.cluster import SpectralClustering, DBSCAN\n",
    "\n",
    "\n",
    "# for ner\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "# for visualization\n",
    "from tqdm import tnrange\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import eli5 # pip install h5py==2.8.0 !!!\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Path to data repertory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data = os.path.join(path_to_repo, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      1. Syntactic Structure\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu_pos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; ;\">\n",
    "      $\\bullet$ Part Of Speech Tags\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " Roles are attributed by ourselve according to the following repartition:\n",
    " \n",
    " | Role | POS tags | Interpretation |\n",
    " |:-----:|:-----|:----|\n",
    " | central | NOUN, PRON, PROPN | core entities |\n",
    " | descriptor | ADJ, ADV, DET, NUM, PART, SYM | descriptors of core entities |\n",
    " | connector | ADP, AUX, CONJ, SCONJ, VERB | links between descriptors/core entities to other descriptors/core entities |\n",
    " | misc | INTJ, PUNCT | junk |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17 Part Of Speech tags for SpaCy models, see the Spacy [documentation](https://universaldependencies.org/docs/u/pos/).\n",
    "\n",
    "| POS Tag | Role | Meaning | Definition |\n",
    "|:-----:|:-----:|:-----:|:-----|\n",
    "|ADJ| descriptor | adjective | <br>Adjectives are words that typically modify nouns and specify their properties or attributes.<br> They may also function as predicates, as in \"The car is green\".<br><br> The ADJ tag is intended for ordinary adjectives only, see DET for determiners and NUM for numerals.<br><br>\n",
    "|ADP| connector | adposition | <br>Adposition is a cover term for prepositions and postpositions. Adpositions belong to a closed set of items that occur before (preposition) or after (postposition) a complement composed of a noun phrase, noun, pronoun, or clause that functions as a noun phrase, and that form a single structure with the complement to express its grammatical and semantic relation to another unit within a clause.<br><br> In many languages, adpositions can take the form of fixed multiword expressions, such as in spite of, because of, thanks to. The component words are then still tagged according to their basic use (in is ADP, spite is NOUN, etc.) and their status as multiword expressions are accounted for in the syntactic annotation.<br><br>\n",
    "|ADV| descriptor | adverb | <br>Adverbs are words that typically modify verbs for such categories as time, place, direction or manner.<br> They may also modify adjectives and other adverbs, as in very briefly or arguably wrong.<br><br> There is a closed subclass of pronominal adverbs that refer to circumstances in context, rather than naming them directly; similarly to pronouns, these can be categorized as interrogative, relative, demonstrative etc.<br> Pronominal adverbs also get the ADV part-of-speech tag but they are differentiated by additional features.<br><br>\n",
    "|AUX| connector | auxiliary verb | <br>An auxiliary verb is a verb that accompanies the lexical verb of a verb phrase and expresses grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect, and voice.<br><br>Modal verbs may count as auxiliaries in some languages (English). In other languages their behavior is not too different from the main verbs and they are thus tagged VERB. Copulas also stay with main verbs.<br><br>\n",
    "|CONJ| connector | coordinating conjunction | <br>A coordinating conjunction is a word that links words or larger constituents without syntactically subordinating one to the other and expresses a semantic relationship between them.<br><br>\n",
    "|CCONJ| connector | coordinating conjunction |\n",
    "|DET| connector | determiner | <br>Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context. That is, a determiner may indicate whether the noun is referring to a definite or indefinite element of a class, to a closer or more distant element, to an element belonging to a specified person or thing, to a particular number or quantity, etc.<br><br>Note that the DET tag includes (pronominal) quantifiers (words like many, few, several), which are included among determiners in some languages but may belong to numerals in others. However, cardinal numerals in the narrow sense (one, five, hundred) are not tagged DET even though some authors would include them in quantifiers. Cardinal numbers have their own tag NUM.<br><br>\n",
    "|INTJ| misc | interjection | <br>An interjection is a word that is used most often as an exclamation or part of an exclamation.<br> It typically expresses an emotional reaction, is not syntactically related to other accompanying expressions, and may include a combination of sounds not otherwise found in the language.<br><br>\n",
    "|NOUN| central | noun | <br>Nouns are a part of speech typically denoting a person, place, thing, animal or idea.<br><br> The NOUN tag is intended for common nouns only. See PROPN for proper nouns and PRON for pronouns.<br><br>\n",
    "|NUM| descriptor | numeral | <br>A numeral is a word, functioning most typically as a determiner, adjective or pronoun, that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.<br><br>Note that cardinal numerals are covered by NUM whether they are used as determiners or not (as in Windows Seven) and whether they are expressed as words (four), digits (4) or Roman numerals (IV). Other words functioning as determiners (including quantifiers such as many and few) are tagged DET.<br><br>\n",
    "|PART| descriptor | particle | <br>Particles are function words that must be associated with another word or phrase to impart meaning and that do not satisfy definitions of other universal parts of speech (e.g. adpositions, coordinating conjunctions, subordinating conjunctions or auxiliary verbs). Particles may encode grammatical categories such as negation, mood, tense etc. Particles are normally not inflected, although exceptions may occur.<br><br>\n",
    "|PRON| central | pronoun |<br>Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable from the linguistic or extralinguistic context.<br><br>Pronouns under this definition function like nouns. Note that some languages traditionally extend the term pronoun to words that substitute for adjectives. Such words are not tagged PRON under our universal scheme. They are tagged as determiners in order to annotate the same thing the same way across languages.<br><br>\n",
    "|PROPN| central | proper noun | <br>A proper noun is a noun (or nominal content word) that is the name (or part of the name) of a specific individual, place, or object.<br><br> Note that PROPN is only used for the subclass of nouns that are used as names and that often exhibit special syntactic properties (such as occurring without an article in the singular in English).<br> When other phrases or sentences are used as names, the component words retain their original tags. For example, in Cat on a Hot Tin Roof, Cat is NOUN, on is ADP, a is DET, etc.<br><br>A fine point is that it is not uncommon to regard words that are etymologically adjectives or participles as proper nouns when they appear as part of a multiword name that overall functions like a proper noun, for example in the Yellow Pages, United Airlines or Thrall Manufacturing Company.<br><br>\n",
    "|PUNCT| misc | punctuation | <br>Punctuation marks are non-alphabetical characters and character groups used in many languages to delimit linguistic units in printed text.<br><br>Punctuation is not taken to include logograms such as \\$, \\%, and §, which are instead tagged as SYM.<br><br>\n",
    "|SCONJ| connector | subordinating conjunction | <br>A subordinating conjunction is a conjunction that links constructions by making one of them a constituent of the other. The subordinating conjunction typically marks the incorporated constituent which has the status of a (subordinate) clause.<br><br>We follow Loos et al. 2003 in recognizing these three subclasses as subordinating conjunctions:<br><br>Complementizers, like [en] that or if Adverbial clause introducers, like [en] when, since, or before (when introducing a clause not a nominal)<br>Relativizers, like [he] še. (Note that these words, which simply introduce a relative caluse, and normally don’t inflect, need to be distinguished from relative or resumptive pronouns, which have a nominal function within the relative clause and which we analyze as PRON.)<br><br>\n",
    "|SYM| descriptor | symbol | <br>A symbol is a word-like entity that differs from ordinary words by form, function, or both.<br><br>Many symbols are or contain special non-alphanumeric characters, similarly to punctuation. What makes them different from punctuation is that they can be substituted by normal words. This involves all currency symbols, e.g. $ 75 is identical to seventy-five dollars.<br><br>Mathematical operators form another group of symbols.<br><br>Another group of symbols is emoticons and emoji.<br><br>Strings that consists entirely of alphanumeric characters are not symbols but they may be proper nouns: 130XE, DC10; others may be tagged PROPN (rather than SYM) even if they contain special characters: DC-10. Similarly, abbreviations for single words are not symbols but are assigned the part of speech of the full form. For example, Mr. (mister), kg (kilogram), km (kilometer), Dr (Doctor) should be tagged nouns. Acronyms for proper names such as UN and NATO should be tagged as proper nouns.<br><br>Characters used as bullets in itemized lists (•, ‣) are not symbols, they are punctuation.<br><br>\n",
    "|VERB| connector | verb | <br>A verb is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause.<br> Verbs are often associated with grammatical categories like tense, mood, aspect and voice, which can either be expressed inflectionally or using auxilliary verbs or particles.<br><br>Note that the VERB tag covers main verbs (content verbs) and copulas but it does not cover auxiliary verbs, for which there is the AUX tag. Modal verbs may be considered VERB or AUX, depending on their behavior in the given language. Language-specific documentation should specify which verbs are tagged AUX in which contexts.<br><br>Note that participles are word forms that may share properties and usage of adjectives and verbs. Depending on language and context, they may be classified as either VERB or ADJ.<br><br>Note that some verb forms such as gerunds and infinitives may share properties and usage of nouns and verbs. Depending on language and context, they may be classified as either VERB or NOUN.<br><br>Note that there are verb forms such as transgressives or adverbial participles that share properties and usage of adverbs and verbs. Depending on language and context, they may be classified as either VERB or ADV.<br><br>\n",
    "|X| misc | other | <br>The tag X is used for words that for some reason cannot be assigned a real part-of-speech category.<br><br>A special usage of X is for cases of code-switching where it is not possible (or meaningful) to analyze the intervening language grammatically (and where the dependency relation foreign is typically used in the syntactic analysis). This usage does not extend to ordinary loan words which should be assigned a normal part-of-speech. For example, in he put on a large sombrero, sombrero is an ordinary NOUN.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu_dep\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; ;\">\n",
    "      $\\bullet$ Dependency Tags\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 47 Dependency tags for French SpaCy models, see the Spacy [documentation](https://universaldependencies.org/treebanks/fr_sequoia/#relations)\n",
    "\n",
    "| Dependency Tag | Meaning | Definition |\n",
    "|:-----:|:-----:|:-----|\n",
    "|acl \n",
    "|acl:relcl\n",
    "|advcl \n",
    "|advcl:cleft \n",
    "|advmod \n",
    "|amod \n",
    "|appos \n",
    "|aux:caus \n",
    "|aux:pass \n",
    "|aux:tense \n",
    "|case \n",
    "|cc \n",
    "|ccomp \n",
    "|conj \n",
    "|cop \n",
    "|csubj \n",
    "|csubj:pass \n",
    "|dep \n",
    "|det \n",
    "|discourse \n",
    "|dislocated \n",
    "|expl:comp \n",
    "|expl:pass \n",
    "|expl:subj \n",
    "|fixed \n",
    "|flat:foreign \n",
    "|flat:name \n",
    "|goeswith \n",
    "|iobj \n",
    "|iobj:agent \n",
    "|mark \n",
    "|nmod \n",
    "|nsubj \n",
    "|nsubj:caus \n",
    "|nsubj:pass \n",
    "|nummod \n",
    "|obj \n",
    "|obj:agent \n",
    "|obl:agent \n",
    "|obl:arg \n",
    "|obl:mod \n",
    "|orphan \n",
    "|parataxis \n",
    "|punct \n",
    "|root \n",
    "|vocative \n",
    "|xcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      2. Natural Language Understanding\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_syntactic_dependency_tree\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Syntactic Dependency Tree\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 MB transformer-based model\n",
    "nlp = spacy.load('fr_dep_news_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_excel(os.path.join(path_to_data, 'data_nuclear_incidents_with_titles.xlsx')).text.tolist()\n",
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = all_texts[1471:1472]\n",
    "#texts = [\"La société Framatome a déclaré le 20 avril 2021à l’Autorité de sûreté nucléaire (ASN) un événement très significatif relatif à la maîtrise du risque de criticité[1].\\nL’événement est survenu dans l’usine de fabrication de combustible située sur la commune de Romans-sur-Isère (Drôme), dans l’installation nucléaire de base (INB) n° 98, dédiée à la fabrication d'éléments combustibles standards utilisés dans les réacteurs à eau sous pression. La matière nucléaire utilisée dans cette installation est de l'uranium à un taux d’enrichissement maximal de 5 %.\"]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(texts[0])\n",
    "\n",
    "displacy.render(doc, style = \"dep\", jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doc_trf_data.wordpieces.strings\n",
    "# doc_trf_data.wordpieces.input_ids\n",
    "# doc_trf_data.wordpieces.lengths\n",
    "# doc_trf_data.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_trf_data = doc._.trf_data\n",
    "doc_trf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection of all syntactic dependency edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for i in tnrange(len(all_texts)):\n",
    "    text = all_texts[i]\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        head = token.head\n",
    "        imin = min([token.i, head.i])\n",
    "        imax = max([token.i, head.i])\n",
    "        if imin < imax:\n",
    "            row = [\n",
    "                token.text, token.pos_, token.tag_, token.dep_, \n",
    "                head.text, head.pos_, head.tag_, \n",
    "                doc[max(0,imin-3): min(imax+3, len(doc))].text,\n",
    "            ]\n",
    "            table.append(row)\n",
    "\n",
    "\n",
    "df_triples = pd.DataFrame(\n",
    "    table, \n",
    "    columns = ['word', 'pos', 'tag', 'dep', 'head word', 'head pos', 'head tag', 'context'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = (df_triples.dep == 'cop')\n",
    "b2 = (df_triples['head pos'] == 'NOUN')\n",
    "b3 = b1 & b2\n",
    "df_triples[b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_triples.to_excel(os.path.join(path_to_data, 'dependency_triples_installations_nucleaires.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu_coref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Coreference resolution\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform multi-word expressions\n",
    "# TODO\n",
    "\n",
    "# perform coreference resolution\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlu_entity_linking\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Entity linking\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply word sense disambiguation\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      3. Knowledge Graph\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_syntactic_dependency_graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Syntactic Dependency Graph\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_roles = {\n",
    "    'subject': ['NOUN', 'PRON', 'PROPN'],\n",
    "    'action': ['VERB'],\n",
    "    'modifier': ['ADJ', 'ADV', 'AUX', 'DET', 'NUM', 'PART', 'SYM', 'PUNCT'],\n",
    "    'connector': ['ADP', 'CONJ', 'SCONJ', 'CCONJ'],\n",
    "    'misc': ['INTJ', 'SPACE'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_dependency_graph(doc, doc_idx = 0):\n",
    "    # init graph\n",
    "    graph = nx.MultiDiGraph()\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    # create nodes\n",
    "    word2count = {}\n",
    "    token2node = {}\n",
    "    for token in doc:\n",
    "        if ((token.pos_ not in POS_roles['misc']) \n",
    "        and (re.sub('[,;:!\\.\\?\\_\\-\\~\\`\\/\\|\\(\\)\\[\\]\\{\\}]', '', token.text) != '')):\n",
    "            word = token.text\n",
    "            word2count[word] = 0 if word not in word2count else word2count[word]+1\n",
    "\n",
    "            # treat different word occurences as different nodes\n",
    "            node_id = '{}_{}_{}'.format(word, doc_idx, word2count[word])\n",
    "\n",
    "            # create and store node\n",
    "            node = {\n",
    "                'id': node_id,\n",
    "                'doc': doc_idx,\n",
    "                'position': token.i,\n",
    "                'word': word,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'dep': token.dep_,\n",
    "                'morphology': token.morph.to_dict(),\n",
    "            }\n",
    "            token2node[token] = node\n",
    "            nodes.append((node_id, node))\n",
    "    graph.add_nodes_from(nodes)\n",
    "\n",
    "    # create edges\n",
    "    for token in doc:\n",
    "        head = token.head\n",
    "        if token in token2node and head in token2node and head != token:\n",
    "            # draw edge in reverse direction !\n",
    "            s_id = token2node[token]['id']\n",
    "            t_id = token2node[head]['id']\n",
    "\n",
    "            if token.i < head.i:\n",
    "                span = doc[token.i+1: head.i]\n",
    "                orient = 'source -> target'\n",
    "            else:\n",
    "                span = doc[head.i+1: token.i]\n",
    "                orient = 'target -> source'\n",
    "\n",
    "            text = span.text\n",
    "            lemma = span.lemma_\n",
    "            dep = token.dep_\n",
    "\n",
    "            # create edge\n",
    "            edge = {\n",
    "                's': s_id,\n",
    "                't': t_id,\n",
    "                'dep': dep,\n",
    "                'text' : text,\n",
    "                'lemma': lemma,\n",
    "                'text orientation': orient,\n",
    "            }\n",
    "            edges.append((s_id, t_id, edge))\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_oriented = get_dependency_graph(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph, node_attribute = 'word'):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(\n",
    "        graph, \n",
    "        pos,\n",
    "        node_color = '#00b4d9', \n",
    "        node_size = 50, \n",
    "        width = 0.25,\n",
    "        edge_cmap = plt.cm.Blues, \n",
    "    )\n",
    "    node_labels = nx.get_node_attributes(graph, node_attribute)\n",
    "    nx.draw_networkx_labels(graph, pos, labels = node_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just not freakin working, as reported at\n",
    "# https://stackoverflow.com/questions/56672123/filenotfounderror-winerror-2-neato-not-found-in-path\n",
    "\n",
    "# G = nx.balanced_tree(3, 5)\n",
    "\n",
    "# pos = nx.drawing.nx_pydot.graphviz_layout(G)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(G, pos, node_size=20, alpha=0.5, node_color=\"blue\", with_labels=False)\n",
    "# plt.axis(\"equal\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_oriented, node_attribute = 'word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_orientation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Edge orientation checking\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_to_revert = ['obj', 'obj:agent', 'nsubj', 'nsubj:caus', 'nsubj:pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_edge_orientation(graph, dep_to_revert):\n",
    "    new_graph = copy.deepcopy(graph)\n",
    "    \n",
    "    for e in graph.edges:\n",
    "        if graph.edges[e]['dep'] in dep_to_revert:\n",
    "            # create new edge\n",
    "            new_e = dict(graph.edges[e])\n",
    "            s, t = new_e['s'], new_e['t']\n",
    "            new_e['s'] = t\n",
    "            new_e['t'] = s\n",
    "            \n",
    "            # replace old edge by newly created one\n",
    "            new_graph.remove_edge(s, t)\n",
    "            new_graph.add_edges_from([(t, s, new_e)])\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_oriented = revert_edge_orientation(graph_oriented, dep_to_revert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_attribute_extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Attribute Extraction from descriptive POS tags\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_paths_from_tree(\n",
    "    doc, \n",
    "    max_distance = None, \n",
    "    min_length = 2,\n",
    "    start_from_leaves = False,\n",
    "    start_pos = None,\n",
    "    trans_pos = None,\n",
    "    stop_pos = None, \n",
    "    dep = None):\n",
    "    '''\n",
    "    Performs bottom-up exploration of the dependency tree of tokens parsed with spacy,\n",
    "    and collect paths in the oriented tree, where token part-of-speeches meet \n",
    "    the requirements and dependency meet the 'dep' requirement,\n",
    "    and that are maximal with respect to these requirements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Iterable.\n",
    "    output of a spacy model applied on a text.\n",
    "\n",
    "    start_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags at the beginning of paths, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    trans_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags during tree walk, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    stop_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags that stop the walk in the tree, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    dep: list, or None.\n",
    "    list of the allowed dependency relations, or None to allow all relations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    completed: list.\n",
    "    The retrieved list of paths meeting the requirements in the dependency tree\n",
    "    '''\n",
    "    completed = []\n",
    "    temporary = [\n",
    "        t for t in doc \n",
    "        if (start_pos is None or t.pos_ in start_pos)\n",
    "        and not (start_from_leaves and set(t.children) & set(temporary))\n",
    "    ]\n",
    "    temporary = [[t] for t in temporary]\n",
    "    \n",
    "    while temporary != []:\n",
    "        checked = []\n",
    "        for chain in temporary:\n",
    "            word = chain[-1]\n",
    "            head = word.head\n",
    "            \n",
    "            # token and its head are different elements\n",
    "            bool1 = (word.dep_ != 'ROOT')\n",
    "            \n",
    "            # distance between last token and its head isn't too high\n",
    "            bool2 = (max_distance is None or abs(word.i - head.i) <= max_distance) \n",
    "            \n",
    "            # dependency fulfills criterion\n",
    "            bool3 = (dep is None or word.dep_ in dep)\n",
    "            \n",
    "            # head fulfills transition criterion\n",
    "            bool4 = (trans_pos is None or head.pos_ in trans_pos)\n",
    "            \n",
    "            # last token doesn't fulfills stopping criterion\n",
    "            bool5 = len(chain) == 1 or not (stop_pos and word.pos_ in stop_pos)\n",
    "            \n",
    "            # accumulation step\n",
    "            if (bool1 and bool2 and bool3 and bool4 and bool5):\n",
    "                checked.append(chain + [head])\n",
    "            else:\n",
    "                completed.append(chain)\n",
    "        temporary = checked\n",
    "\n",
    "    completed = [c for c in completed if len(c) >= min_length]\n",
    "    return completed\n",
    "\n",
    "\n",
    "\n",
    "def get_dependency_paths_from_graph(\n",
    "    graph, \n",
    "    max_distance = None, \n",
    "    min_length = 2,\n",
    "    start_from_leaves = False,\n",
    "    start_pos = None,\n",
    "    trans_pos = None,\n",
    "    stop_pos = None, \n",
    "    dep = None):\n",
    "    '''\n",
    "    Performs bottom-up exploration of a dependency graph,\n",
    "    and collect paths in the oriented graph, where token part-of-speeches meet \n",
    "    the requirements and dependency meet the 'dep' requirement,\n",
    "    and that are maximal with respect to these requirements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: nx.DiGraph object.\n",
    "\n",
    "    start_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags at the beginning of paths, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    trans_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags during tree walk, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    stop_pos: list, or None.\n",
    "    list of the allowed part-of-speech tags that stop the walk in the tree, \n",
    "    or None to allow all tags.\n",
    "\n",
    "    dep: list, or None.\n",
    "    list of the allowed dependency relations, or None to allow all relations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    completed: list.\n",
    "    The retrieved list of paths meeting the requirements in the dependency graph\n",
    "    '''\n",
    "    \n",
    "    completed = []\n",
    "    temporary = [\n",
    "        n for n in graph.nodes \n",
    "        if (start_pos is None or graph.nodes[n]['pos'] in start_pos)\n",
    "        and (not start_from_leaves or list(graph.predecessors(n)) == [])\n",
    "    ]\n",
    "    temporary = [[t] for t in temporary]\n",
    "    \n",
    "    while temporary != []:\n",
    "        checked = []\n",
    "        for chain in temporary:\n",
    "            word = chain[-1]\n",
    "            head = list(graph.successors(word))\n",
    "            if head: head = head[0]\n",
    "            \n",
    "            # token has a head\n",
    "            bool1 = (head != [])\n",
    "            \n",
    "            # distance between last token and its head isn't too high\n",
    "            bool2 = bool1 and (max_distance is None or abs(graph.nodes[word]['position'] - graph.nodes[head]['position']) <= max_distance) \n",
    "            \n",
    "            # dependency fulfills criterion\n",
    "            bool3 = (dep is None or graph.nodes[word]['dep'] in dep)\n",
    "            \n",
    "            # head fulfills transition criterion\n",
    "            bool4 = bool1 and (trans_pos is None or graph.nodes[head]['pos'] in trans_pos)\n",
    "            \n",
    "            # last token doesn't fulfills stopping criterion\n",
    "            bool5 = len(chain) == 1 or not (stop_pos and graph.nodes[word]['pos'] in stop_pos)\n",
    "            \n",
    "            # accumulation step\n",
    "            if (bool1 and bool2 and bool3 and bool4 and bool5):\n",
    "                checked.append(chain + [head])\n",
    "            else:\n",
    "                completed.append(chain)\n",
    "        temporary = checked\n",
    "        \n",
    "    completed = [c for c in completed if len(c) >= min_length]\n",
    "    return completed\n",
    "\n",
    "\n",
    "\n",
    "def convert_paths_to_attributes(graph, paths, attribute_name, remove_paths = False):\n",
    "    graph = copy.deepcopy(graph)\n",
    "    nouns = set(list([p[-1] for p in paths]))\n",
    "    attrs = set(list([w for p in paths for w in p[:-1]]))\n",
    "    noun2path = {n: list([p[:-1] for p in paths if p[-1] == n]) for n in nouns}\n",
    "    \n",
    "    # add dependency paths as attribute to central nodes\n",
    "    for noun in nouns:\n",
    "        graph.nodes[noun][attribute_name] = noun2path[noun]\n",
    "    \n",
    "    # remove dependency paths from graph\n",
    "    if remove_paths:\n",
    "        for attr in attrs:\n",
    "            graph.remove_node(attr)\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "def add_attribute_spans(graph, doc, attribute_name, attribute_alt = None):\n",
    "    graph = copy.deepcopy(graph)\n",
    "    for n in graph.nodes:\n",
    "        node = graph.nodes[n]\n",
    "        if attribute_name in node:\n",
    "            ids = [n] + [m for p in node[attribute_name] for m in p]\n",
    "            ids = [graph.nodes[m]['position'] for m in ids]\n",
    "            imin = min(ids)\n",
    "            imax = max(ids)\n",
    "            span = doc[imin: imax+1].text\n",
    "            graph.nodes[n][attribute_name + ' span'] = span\n",
    "        elif attribute_alt in node:\n",
    "            graph.nodes[n][attribute_name + ' span'] = graph.nodes[n][attribute_alt]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = get_dependency_paths_from_graph(\n",
    "    graph_oriented, \n",
    "    max_distance = None, \n",
    "    min_length = 2,\n",
    "    start_from_leaves = True,\n",
    "    start_pos = POS_roles['modifier'] + POS_roles['connector'] + POS_roles['misc'], # None\n",
    "    trans_pos = None, \n",
    "    stop_pos  = POS_roles['subject'] + POS_roles['action'],\n",
    "    dep = None,\n",
    ")\n",
    "graph_oriented_with_attributes = convert_paths_to_attributes(\n",
    "    graph_oriented, \n",
    "    attributes, \n",
    "    attribute_name = 'description',\n",
    ")\n",
    "graph_oriented_with_attributes = add_attribute_spans(\n",
    "    graph_oriented_with_attributes, \n",
    "    doc, \n",
    "    attribute_name = 'description',\n",
    "    attribute_alt = 'word',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By slightly modifying the path reconstruction process, we (almost) end up with the chunks provided by the SpaCy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_attributes = get_dependency_paths_from_graph(\n",
    "    graph_oriented, \n",
    "    max_distance = None, \n",
    "    min_length = 2,\n",
    "    start_from_leaves = True,\n",
    "    start_pos = POS_roles['subject'] + POS_roles['modifier'],\n",
    "    trans_pos = POS_roles['subject'] + POS_roles['modifier'],\n",
    "    stop_pos  = None,\n",
    "    dep = None,\n",
    ")\n",
    "wide_attributes = [\n",
    "    p for p in wide_attributes \n",
    "    if graph_oriented.nodes[p[-1]]['pos'] in POS_roles['subject']\n",
    "    and re.sub('[^a-zA-Z0-9]', '', graph_oriented.nodes[p[-1]]['word']) != ''\n",
    "]\n",
    "graph_oriented_with_attributes = convert_paths_to_attributes(\n",
    "    graph_oriented_with_attributes, \n",
    "    wide_attributes,\n",
    "    attribute_name = 'wide description',\n",
    ")\n",
    "graph_oriented_with_attributes = add_attribute_spans(\n",
    "    graph_oriented_with_attributes, \n",
    "    doc, \n",
    "    attribute_name = 'wide description',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in graph_oriented_with_attributes.nodes:\n",
    "    node = graph_oriented_with_attributes.nodes[n]\n",
    "    if 'wide description span' in node:\n",
    "        print(node['wide description span'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_oriented_with_attributes, node_attribute = 'word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_relation_extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "          $\\bullet$ Relation Extraction from Connecting POS tags\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_paths_to_relations(graph, paths, remove_paths = False):\n",
    "    def path_to_relation(path):\n",
    "        return list(path[1:-1])\n",
    "    \n",
    "    graph = copy.deepcopy(graph)\n",
    "    edges = [(p[0], p[-1], tuple(path_to_relation(p))) for p in paths]\n",
    "    graph.add_edges_from(edges)\n",
    "        \n",
    "    if remove_paths:\n",
    "        nodes = set([p[i] for p in paths for i in [0, -1]])\n",
    "        temps = set([n for p in paths for n in p[1:-1]]) - nodes\n",
    "        nodes = list(nodes)\n",
    "        temps = list(temps)\n",
    "        for temp in temps:\n",
    "            graph.remove_node(temp)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = get_dependency_paths_from_graph(\n",
    "    graph_oriented_with_attributes, \n",
    "    max_distance = None, \n",
    "    start_from_leaves = False,\n",
    "    start_pos = POS_roles['subject'],\n",
    "    trans_pos = None, \n",
    "    stop_pos  = POS_roles['subject'] + POS_roles['action'],\n",
    "    min_length = 3,\n",
    "    dep = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_oriented_with_attributes_and_relations = convert_paths_to_relations(graph_oriented_with_attributes, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_oriented_with_attributes_and_relations, node_attribute = 'word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_relation_classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Relation Classification\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ is-a<br>\n",
    "$\\bullet$ part-whole<br>\n",
    "$\\bullet$ property<br>\n",
    "$\\bullet$ value<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_drop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Drop off uneccessary nodes\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nodes(graph, pos):\n",
    "    graph = copy.deepcopy(graph)\n",
    "    temps = [n for n in graph.nodes if graph.nodes[n]['pos'] in pos]\n",
    "    for n in temps:\n",
    "        graph.remove_node(n)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = remove_nodes(\n",
    "    graph_oriented_with_attributes_and_relations, \n",
    "    pos = POS_roles['modifier'] + POS_roles['connector'] + POS_roles['misc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph, node_attribute = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph, node_attribute = 'description span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pos = [graph.nodes[n]['pos'] for n in graph.nodes]\n",
    "graph_plt = pd.DataFrame({'POS': graph_pos}).groupby(['POS']).apply(len).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplement : Only display subject nodes that are maximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_graph(graph):\n",
    "    graph = copy.deepcopy(graph)\n",
    "    temps = [\n",
    "        n for n in graph.nodes \n",
    "        if graph.nodes[n]['pos'] != 'VERB'\n",
    "        and 'wide description span' not in graph.nodes[n]\n",
    "    ]\n",
    "    print(temps)\n",
    "    for n in temps:\n",
    "        graph.remove_node(n)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_max, node_attribute = 'wide description span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_quotient\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Quotient under \"same word and POS\" equivalence relation\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equiv_relation(u, v):\n",
    "    same_word = graph.nodes[u]['word'] == graph.nodes[v]['word'] \n",
    "    same_pos  = graph.nodes[u]['pos'] == graph.nodes[v]['pos']\n",
    "    return (same_word and same_pos)\n",
    "\n",
    "\n",
    "def equiv_classes_data(b):\n",
    "    S = graph.subgraph(b)\n",
    "    nodes = list(S.nodes)\n",
    "    nodes.sort(key = lambda n: int(n.split('_')[-1]))\n",
    "    node = nodes[0]\n",
    "    data = {\n",
    "        # default\n",
    "        'graph': S, \n",
    "        'nnodes': len(S), \n",
    "        'nedges': S.number_of_edges(), \n",
    "        'density': nx.density(S),\n",
    "        \n",
    "        # custom\n",
    "        'word': S.nodes[node]['word'],\n",
    "        'pos': S.nodes[node]['pos'],\n",
    "        'description span': S.nodes[node]['description span'],\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_quotient = nx.quotient_graph(\n",
    "    graph, \n",
    "    partition = equiv_relation,\n",
    "    node_data = equiv_classes_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_quotient, node_attribute = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_quotient, node_attribute = 'description span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kg_construction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: left; \n",
    "      padding: 0px; \n",
    "      margin: 0px;\">\n",
    "      $\\bullet$ Final Graph construction\n",
    "  </div> \n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph_from_doc(doc, doc_idx = 0):\n",
    "    # build GOW\n",
    "    graph = get_dependency_graph(doc, doc_idx = doc_idx)\n",
    "    \n",
    "    # compute and merge subject attributes\n",
    "    attributes = get_dependency_paths_from_graph(\n",
    "        graph, \n",
    "        max_distance = None, \n",
    "        min_length = 2,\n",
    "        start_from_leaves = True,\n",
    "        start_pos = None,\n",
    "        trans_pos = None, \n",
    "        stop_pos  = POS_roles['subject'] + POS_roles['action'],\n",
    "        dep = None,\n",
    "    )\n",
    "    graph = convert_paths_to_attributes(\n",
    "        graph, \n",
    "        attributes, \n",
    "        attribute_name = 'description',\n",
    "    )\n",
    "    # add text span of each subject\n",
    "    graph = add_attribute_spans(\n",
    "        graph, \n",
    "        doc, \n",
    "        attribute_name = 'description',\n",
    "        attribute_alt = 'word',\n",
    "    )\n",
    "    # compute and merge wider subject attributes\n",
    "    wide_attributes = get_dependency_paths_from_graph(\n",
    "        graph, \n",
    "        max_distance = None, \n",
    "        min_length = 2,\n",
    "        start_from_leaves = True,\n",
    "        start_pos = POS_roles['subject'] + POS_roles['modifier'],\n",
    "        trans_pos = POS_roles['subject'] + POS_roles['modifier'],\n",
    "        stop_pos  = None,\n",
    "        dep = None,\n",
    "    )\n",
    "    wide_attributes = [\n",
    "        p for p in wide_attributes \n",
    "        if graph.nodes[p[-1]]['pos'] in POS_roles['subject']\n",
    "        and re.sub('[^a-zA-Z0-9]', '', graph.nodes[p[-1]]['word']) != ''\n",
    "    ]\n",
    "    graph = convert_paths_to_attributes(graph, wide_attributes, attribute_name = 'wide description')\n",
    "    \n",
    "    # add wider text span of each subject\n",
    "    graph = add_attribute_spans(graph, doc, attribute_name = 'wide description')\n",
    "    \n",
    "    # compute and merge relations\n",
    "    relations = get_dependency_paths_from_graph(\n",
    "        graph, \n",
    "        max_distance = None, \n",
    "        start_from_leaves = False,\n",
    "        start_pos = POS_roles['subject'],\n",
    "        trans_pos = None, \n",
    "        stop_pos  = POS_roles['subject'] + POS_roles['action'],\n",
    "        min_length = 3,\n",
    "        dep = None,\n",
    "    )\n",
    "    graph = convert_paths_to_relations(graph, relations)\n",
    "    \n",
    "    # remove uneccessary nodes\n",
    "    graph = remove_nodes(graph, pos = POS_roles['modifier'] + POS_roles['connector'] + POS_roles['misc'])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_quotient_graph(graph):\n",
    "    '''\n",
    "    Get quotient graph by merging nodes with same word and pos.\n",
    "    '''\n",
    "    def equiv_relation(u, v):\n",
    "        same_word = graph.nodes[u]['word'] == graph.nodes[v]['word'] \n",
    "        same_pos  = graph.nodes[u]['pos'] == graph.nodes[v]['pos']\n",
    "        return (same_word and same_pos)\n",
    "\n",
    "    def equiv_classes_node(b):\n",
    "        S = graph.subgraph(b)\n",
    "        nodes = list(S.nodes)\n",
    "        nodes.sort(key = lambda n: int(n.split('_')[-1]))\n",
    "        node = nodes[0]\n",
    "        data = {\n",
    "            # default\n",
    "            'graph': S, \n",
    "            'nnodes': len(S), \n",
    "            'nedges': S.number_of_edges(), \n",
    "            'density': nx.density(S),\n",
    "\n",
    "            # custom\n",
    "            'id': S.nodes[node]['word'] + '_' + S.nodes[node]['pos'],\n",
    "            'word': S.nodes[node]['word'],\n",
    "            'pos': S.nodes[node]['pos'],\n",
    "            'description span': S.nodes[node]['description span'],\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "    def equiv_classes_edge(e, f):\n",
    "\n",
    "        data = {\n",
    "            # custom\n",
    "            'text': graph.edges(e, f)['word'],\n",
    "            'pos': S.nodes[node]['pos'],\n",
    "            'description span': S.nodes[node]['description span'],\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "    graph = nx.quotient_graph(\n",
    "        graph, \n",
    "        partition = equiv_relation, \n",
    "        node_data = equiv_classes_node,\n",
    "        edge_data = equiv_classes_edge,\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "\n",
    "def build_knowledge_graph_from_corpus(texts, nlp, start_idx = 0):\n",
    "    # fill graph\n",
    "    graph = nx.MultiDiGraph()\n",
    "    for i in tnrange(len(texts)):\n",
    "        text = texts[i]\n",
    "        doc = nlp(text)\n",
    "        new_graph = build_knowledge_graph_from_doc(doc, doc_idx = i+start_idx)\n",
    "        graph = nx.compose(graph, new_graph)\n",
    "        \n",
    "    # get quotient graph by merging nodes with same word and pos\n",
    "    graph = get_quotient_graph(graph)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_2 = build_knowledge_graph_from_corpus(all_texts[:2], nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_2, node_attribute = 'description span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_5 = build_knowledge_graph_from_corpus(all_texts[:5], nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(graph_5, node_attribute = 'description span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bottom\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of content](#TOC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
