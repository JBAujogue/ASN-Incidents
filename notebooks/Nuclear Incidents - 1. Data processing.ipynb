{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Nuclear Incidents\n",
    "  </div> \n",
    "\n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Data Preparation\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\" float:left; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> \n",
    "  \n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jan 2023\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Table of Content\n",
    "  </div> \n",
    "\n",
    "1. [Corpus Import](#corpus)\n",
    "2. [Metadata Extraction](#metadata)\n",
    "3. [Text Segmentation](#segmentation)\n",
    "\n",
    "\n",
    "\n",
    "[Bottom](#bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import unicodedata\n",
    "import re\n",
    "import copy\n",
    "\n",
    "# data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nlp\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "\n",
    "# visualization\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data_raw = os.path.join(path_to_repo, 'data', 'raw')\n",
    "path_to_data_prc = os.path.join(path_to_repo, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(path_to_repo, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtools.tfidf import compute_gensim_tfidf_similarity_matrix\n",
    "from tmtools.span import get_maximal_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relevant nlp models:\n",
    "\n",
    "# 450 MB transformer-based model\n",
    "nlp_en = spacy.load('en_core_web_trf')\n",
    "\n",
    "# Merge multi-word entities into single tokens\n",
    "nlp_en.add_pipe(\"merge_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 MB transformer-based model\n",
    "nlp = spacy.load('fr_dep_news_trf', disable = ['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Corpus Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_incidents = pd.read_excel(os.path.join(path_to_data_raw, 'data_nuclear_incidents.xlsx'))\n",
    "df_incidents = df_incidents[['text']]\n",
    "df_incidents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metadata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Metadata Extraction\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents['Doc_id'] = df_incidents.index.tolist()\n",
    "df_incidents['title']  = df_incidents['text'].apply(lambda t: t.split('\\n')[0])\n",
    "df_incidents['text']   = df_incidents['text'].apply(lambda t: '\\n'.join(t.split('\\n')[1:]))\n",
    "\n",
    "df_incidents = df_incidents[['Doc_id', 'text', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dates and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# an example\n",
    "text = df_incidents.text[1]\n",
    "doc = nlp_en(text)\n",
    "\n",
    "spacy.displacy.render(doc, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_incidents['title'].tolist()\n",
    "texts  = df_incidents.text.tolist()\n",
    "\n",
    "common_entities = []\n",
    "for i in trange(len(texts)):\n",
    "    title = titles[i]\n",
    "    text = texts[i]\n",
    "    ents = nlp_en(title + '\\n' + text).ents\n",
    "    common_entities.append(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [[ent.text for ent in ents if ent.label_ == 'DATE'] for ents in common_entities]\n",
    "dates = [ds[0].replace('2021Ã ', '2021') if len(ds)>0 else None for ds in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents['date'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [[ent for ent in ents if ent.label_ == 'GPE'] for ents in common_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_locations = [loc.text for locs in locations for loc in locs]\n",
    "unique_locations += ['Saint-Laurent-des-Eaux']\n",
    "unique_locations = set(unique_locations)\n",
    "unique_locations - set(['Areva NC', 'Aucun', 'Saint'])\n",
    "unique_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_locations = [[(loc, text.count(loc)) for loc in unique_locations] for text in df_incidents.text]\n",
    "infered_locations = [sorted(locs, key = lambda lc: lc[1], reverse = True)[0] for locs in infered_locations]\n",
    "infered_locations = [lc[0] if lc[1]>0 else 'Inconnu' for lc in infered_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([loc for loc in infered_locations if loc == 'Inconnu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents['location'] = infered_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents = df_incidents[['Doc_id', 'title', 'date', 'location', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.to_excel(os.path.join(path_to_data_prc, 'source_texts.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segmentation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text segmentation\n",
    "\n",
    "[Table of Content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents = pd.read_excel(os.path.join(path_to_data_prc, 'source_texts.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Segmentation into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = df_incidents.apply(\n",
    "    func = lambda r: [(r.Doc_id, i, r.title, r.date, r.location, p) for i, p in enumerate(re.sub('(\\n)+', '\\n', r.text).split('\\n'))],\n",
    "    axis = 1,\n",
    ")\n",
    "paragraphs = [p for ps in paragraphs for p in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs = pd.DataFrame(paragraphs, columns = ['Doc_id', 'Para_id', 'title', 'date', 'location', 'text'])\n",
    "df_paragraphs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs.to_excel(os.path.join(path_to_data_prc, 'source_paragraphs.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Segmentation into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_paragraphs.apply(\n",
    "    func = lambda r: [(r.Doc_id, r.Para_id, i, r.title, r.date, r.location, s) for i, s in enumerate(tokenizer.tokenize(r.text))],\n",
    "    axis = 1,\n",
    ")\n",
    "sentences = [s for ss in sentences for s in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.DataFrame(sentences, columns = ['Doc_id', 'Para_id', 'Sent_id', 'title', 'date', 'location', 'text'])\n",
    "df_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.to_excel(os.path.join(path_to_data_prc, 'source_sentences.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Span extraction\n",
    "\n",
    "[Table of Content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_data(s):\n",
    "    return (s.text, s.lemma_, s.root.lemma_, s.start, s.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = df_sentences.apply(\n",
    "    func = lambda r: [\n",
    "        [r.Doc_id, r.Para_id, r.Sent_id, i, r.title, r.date, r.location] + list(get_span_data(s)) \n",
    "        for i, s in enumerate(get_maximal_spans(nlp(r.text)))\n",
    "    ],\n",
    "    axis = 1,\n",
    ")\n",
    "spans = [s for ss in spans for s in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spans = pd.DataFrame(spans, columns = [\n",
    "    'Doc_id', 'Para_id', 'Sent_id', 'Span_id', 'title', 'date', 'location', 'text', 'lemma', 'root', 'start', 'end',\n",
    "])\n",
    "df_spans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spans.to_excel(os.path.join(path_to_data_prc, 'source_spans.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bottom\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of content](#TOC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
